import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import StratifiedKFold # Import for K-Fold Cross-Validation
import os
import pandas as pd
from pathlib import Path
import json
import pickle
from typing import Tuple, List, Dict, Any
import warnings
import shutil # Import shutil for copying directories
from google.colab import drive # Import google.colab.drive for mounting Google Drive
import zipfile # Import zipfile for unzipping

warnings.filterwarnings('ignore')

class PharynxImagePreprocessor:
    """Advanced image preprocessing for pharynx/throat images"""
    
    def __init__(self, target_size=(224, 224)):
        self.target_size = target_size
    
    def enhance_image(self, image: np.ndarray) -> np.ndarray:
        """Apply image enhancement techniques specific to pharynx visualization"""
        # Convert to LAB color space for better color processing
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        l, a, b = cv2.split(lab)
        
        # Apply CLAHE to L channel for better contrast
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        l = clahe.apply(l)
        
        # Merge channels and convert back
        enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)
        
        # Additional sharpening for pharynx detail
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(enhanced, -1, kernel)
        enhanced = cv2.addWeighted(enhanced, 0.7, sharpened, 0.3, 0)
        
        return enhanced
    
    def extract_inflammation_features(self, image: np.ndarray) -> Dict[str, float]:
        """Extract inflammation features specific to pharyngitis"""
        # Convert to multiple color spaces for comprehensive analysis
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        
        # Define red color ranges for pharyngeal inflammation
        # These ranges might need fine-tuning based on your specific dataset's color characteristics
        lower_red1 = np.array([0, 40, 40])
        upper_red1 = np.array([15, 255, 255])
        lower_red2 = np.array([165, 40, 40])
        upper_red2 = np.array([180, 255, 255])
        
        # Create masks for inflamed regions
        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
        inflammation_mask = cv2.bitwise_or(mask1, mask2)
        
        # Calculate inflammation metrics
        total_pixels = image.shape[0] * image.shape[1]
        inflamed_pixels = cv2.countNonZero(inflammation_mask)
        inflammation_ratio = inflamed_pixels / total_pixels
        
        # Analyze color intensity in A channel (green-red axis)
        a_channel = lab[:, :, 1]
        # Consider only values above a certain threshold to focus on 'redder' areas
        red_intensity = np.mean(a_channel[a_channel > 128])  # Red values in A channel (typically > 128)
        
        # Calculate texture roughness (indicator of inflammation) using Laplacian variance
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        return {
            'inflammation_ratio': inflammation_ratio,
            'red_intensity': red_intensity if not np.isnan(red_intensity) else 0, # Handle cases where no red pixels are found
            'texture_roughness': laplacian_var
        }
    
    def detect_pharyngeal_patches(self, image: np.ndarray) -> Tuple[int, float, float]:
        """Detect white/yellowish patches, exudate, or abnormal coloring in pharynx"""
        # Convert to LAB for better white detection (L channel for lightness)
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        l_channel = lab[:, :, 0]
        
        # Detect white/bright patches (exudate) using a high lightness threshold
        _, white_mask = cv2.threshold(l_channel, 170, 255, cv2.THRESH_BINARY)
        
        # Detect yellowish patches using HSV color space
        # Corrected: Convert from RGB to HSV, not HSV to RGB
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV) 
        # Define yellow color range in HSV
        lower_yellow = np.array([15, 50, 50])
        upper_yellow = np.array([35, 255, 255])
        yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)
        
        # Combine masks for all types of patches
        patch_mask = cv2.bitwise_or(white_mask, yellow_mask)
        
        # Find contours of detected patches
        contours, _ = cv2.findContours(patch_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Filter contours by a minimum area to remove small noise
        min_area = 30 # This value might need adjustment based on image resolution
        valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]
        
        patch_count = len(valid_contours)
        total_patch_area = sum([cv2.contourArea(c) for c in valid_contours])
        
        # Calculate ratio of patched area to total image area
        total_image_area = image.shape[0] * image.shape[1]
        patch_ratio = total_patch_area / total_image_area
        
        # Calculate average patch size
        avg_patch_size = total_patch_area / patch_count if patch_count > 0 else 0
        
        return patch_count, patch_ratio, avg_patch_size
    
    def analyze_pharyngeal_swelling(self, image: np.ndarray) -> float:
        """Analyze swelling patterns in pharyngeal tissue by assessing edge complexity"""
        # Convert to grayscale for structural analysis
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Apply Gaussian blur to reduce noise and help Canny edge detection
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # Detect edges using Canny algorithm
        edges = cv2.Canny(blurred, 50, 150) # Thresholds might need tuning
        
        # Find contours from the detected edges
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Analyze contour complexity: more complex/irregular contours can indicate swelling
        total_perimeter = sum([cv2.arcLength(contour, True) for contour in contours])
        total_area = sum([cv2.contourArea(contour) for contour in contours])
        
        # Calculate a complexity ratio: higher ratio for more convoluted shapes
        # Add a small epsilon to total_area to prevent division by zero
        complexity_ratio = total_perimeter / (total_area + 1e-6)
        
        return complexity_ratio
    
    def preprocess_image(self, image: np.ndarray) -> Tuple[np.ndarray, Dict[str, float]]:
        """Complete preprocessing pipeline for pharyngitis detection"""
        # Resize image to target_size
        resized = cv2.resize(image, self.target_size)
        
        # Enhance image for better visual quality and feature extraction
        enhanced = self.enhance_image(resized)
        
        # Extract inflammation-related features
        inflammation_features = self.extract_inflammation_features(enhanced)
        
        # Detect patches and exudate, and get their metrics
        patch_count, patch_ratio, avg_patch_size = self.detect_pharyngeal_patches(enhanced)
        
        # Analyze swelling patterns
        swelling_complexity = self.analyze_pharyngeal_swelling(enhanced)
        
        # Normalize pixel values to [0, 1] for neural network input
        normalized = enhanced.astype(np.float32) / 255.0
        
        # Combine all extracted features into a single dictionary
        features = {
            **inflammation_features,
            'patch_count': float(patch_count), # Ensure numerical type
            'patch_ratio': patch_ratio,
            'avg_patch_size': avg_patch_size,
            'swelling_complexity': swelling_complexity
        }
        
        return normalized, features

# New class for handling dual input data generation
class DualInputDataGenerator(keras.utils.Sequence):
    """
    Custom Keras Sequence for generating batches with dual inputs (image and extracted features).
    Wraps an ImageDataGenerator to provide image batches and computes features on the fly.
    """
    def __init__(self, image_generator: ImageDataGenerator, preprocessor: PharynxImagePreprocessor):
        self.image_generator = image_generator
        self.preprocessor = preprocessor

    def __len__(self):
        """Returns the number of batches per epoch."""
        return len(self.image_generator)

    def __getitem__(self, idx: int) -> Tuple[List[np.ndarray], np.ndarray]:
        """Generates one batch of data."""
        # Get image batch and labels from the underlying ImageDataGenerator
        img_batch, label_batch = self.image_generator[idx]
        
        feature_batch = []
        # Iterate through each image in the batch to extract features
        for i in range(img_batch.shape[0]):
            # Convert image from normalized float32 to uint8 for OpenCV processing
            original_image_uint8 = (img_batch[i] * 255).astype(np.uint8)
            # Preprocess and extract features for the current image
            _, features = self.preprocessor.preprocess_image(original_image_uint8)
            # Order of features must match the model's feature_input layer
            feature_vector = [
                features['inflammation_ratio'],
                features['red_intensity'],
                features['texture_roughness'],
                features['patch_count'],
                features['patch_ratio'],
                features['swelling_complexity']
            ]
            feature_batch.append(feature_vector)
            
        # Return both inputs (image batch and feature batch) and the labels
        # Changed from list to tuple for inputs to resolve TypeError
        return (img_batch, np.array(feature_batch)), label_batch

    def on_epoch_end(self):
        """Called at the end of each epoch, useful for shuffling."""
        # Reset the underlying ImageDataGenerator to shuffle data if configured
        self.image_generator.on_epoch_end()
        # For flow_from_directory, reset() is usually sufficient for shuffling
        # self.image_generator.reset() # This is handled by on_epoch_end for flow_from_directory
        
class PharyngitisDetectionModel:
    """Main model class for pharyngitis detection"""
    
    def __init__(self, num_classes=2, input_shape=(224, 224, 3)): # Changed num_classes to 2
        self.num_classes = num_classes
        self.input_shape = input_shape
        self.model = None
        self.preprocessor = PharynxImagePreprocessor()
        # self.class_names will be set dynamically after data generators are created
        self.class_names = [] 
        
    def build_model(self) -> keras.Model:
        """Build the CNN model architecture optimized for pharyngitis detection"""
        # Use EfficientNetB0 as base model for image feature extraction
        base_model = applications.EfficientNetB0(
            weights='imagenet', # Use pre-trained weights from ImageNet
            include_top=False,  # Exclude the top classification layer
            input_shape=self.input_shape
        )
        
        # Freeze base model initially to train only the new layers
        base_model.trainable = False
        
        # Custom feature extraction branch for medical features (6 features: inflammation_ratio, red_intensity, texture_roughness, patch_count, patch_ratio, swelling_complexity)
        feature_input = layers.Input(shape=(6,), name='medical_features')
        # Added L2 regularization.
        # It's important to ensure these medical features are appropriately scaled (e.g., Min-Max or Standard Scaling)
        # before being fed into the model, even with BatchNormalization.
        feature_dense = layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(feature_input)
        feature_dense = layers.BatchNormalization()(feature_dense)
        feature_dense = layers.Dropout(0.3)(feature_dense) # Dropout for regularization
        
        # Image processing branch
        image_input = layers.Input(shape=self.input_shape, name='image_input')
        image_features = base_model(image_input) # Pass image through EfficientNet
        image_features = layers.GlobalAveragePooling2D()(image_features) # Reduce spatial dimensions
        image_features = layers.BatchNormalization()(image_features)
        image_features = layers.Dropout(0.4)(image_features)
        # Added L2 regularization
        image_features = layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(image_features)
        image_features = layers.BatchNormalization()(image_features)
        image_features = layers.Dropout(0.3)(image_features)
        
        # Combine image and feature branches
        combined = layers.concatenate([image_features, feature_dense])
        # Added L2 regularization
        combined = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(combined)
        combined = layers.BatchNormalization()(combined)
        combined = layers.Dropout(0.3)(combined)
        # Added L2 regularization
        combined = layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(combined)
        combined = layers.Dropout(0.2)(combined)
        
        # Output layer with softmax activation for multi-class classification
        predictions = layers.Dense(self.num_classes, activation='softmax', name='predictions')(combined)
        
        # Create the Keras Model with two inputs and one output
        model = keras.Model(inputs=[image_input, feature_input], outputs=predictions)
        
        # Compile model with Adam optimizer and categorical crossentropy loss
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'precision', 'recall'] # Include precision and recall for medical tasks
        )
        
        self.model = model
        return model
    
    def create_data_generators(self, train_dir: str, val_dir: str, batch_size: int = 32):
        """
        Create data generators with augmentation for pharyngitis images.
        This method will now return instances of DualInputDataGenerator.
        """
        # Training data augmentation - more conservative for medical images
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=10,
            width_shift_range=0.08,
            height_shift_range=0.08,
            shear_range=0.05,
            zoom_range=0.08,
            horizontal_flip=True,
            brightness_range=[0.85, 1.15],
            fill_mode='nearest'
        )
        
        # Validation data (only rescaling)
        val_datagen = ImageDataGenerator(rescale=1./255)
        
        # Flow from directory for images
        train_image_generator = train_datagen.flow_from_directory(
            train_dir,
            target_size=self.input_shape[:2],
            batch_size=batch_size,
            class_mode='categorical',
            shuffle=True
        )
        
        val_image_generator = val_datagen.flow_from_directory(
            val_dir,
            target_size=self.input_shape[:2],
            batch_size=batch_size,
            class_mode='categorical',
            shuffle=False
        )
        
        # Dynamically set class_names based on what ImageDataGenerator found
        # This ensures correct mapping even with unusual directory names like 'no (1)'
        self.class_names = sorted(train_image_generator.class_indices.keys())

        # Instantiate the new DualInputDataGenerator class
        train_dual_gen = DualInputDataGenerator(train_image_generator, self.preprocessor)
        val_dual_gen = DualInputDataGenerator(val_image_generator, self.preprocessor)

        return train_dual_gen, val_dual_gen
    
    def train_model(self, train_generator, val_generator, epochs: int = 60):
        """Train the model with callbacks optimized for medical imaging"""
        # Define callbacks for training
        callbacks = [
            EarlyStopping(
                monitor='val_loss', # Changed to monitor validation loss
                patience=5,          # Adjusted patience as per recommendation
                restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity.
                verbose=1
            ),
            ReduceLROnPlateau( # Corrected typo here
                monitor='val_loss', # Monitor validation loss
                factor=0.3,         # Factor by which the learning rate will be reduced. new_lr = lr * factor
                patience=6,         # Number of epochs with no improvement after which learning rate will be reduced.
                min_lr=1e-8,        # Lower bound on the learning rate.
                verbose=1
            ),
            ModelCheckpoint(
                'best_pharyngitis_model.h5', # Path to save the model file
                monitor='val_loss',     # Changed to save model based on validation loss
                save_best_only=True,        # Only save when val_loss improves
                verbose=1
            )
        ]
        
        print("Starting initial training phase...")
        # Initial training phase (base model frozen)
        history = self.model.fit(
            train_generator,
            epochs=epochs//2, # Train for half the total epochs
            validation_data=val_generator,
            callbacks=callbacks,
            verbose=1
        )
        
        print("\nStarting fine-tuning phase: Unfreezing base model layers...")
        # Fine-tuning phase: Unfreeze the base model
        # The base_model (EfficientNetB0) is the 3rd layer in the combined model (index 2)
        self.model.layers[2].trainable = True
        
        # Recompile the model with a lower learning rate for fine-tuning
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Smaller learning rate for fine-tuning
            loss='categorical_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        # Continue training for the remaining epochs
        history_fine = self.model.fit(
            train_generator,
            epochs=epochs//2, # Train for the other half of the total epochs
            validation_data=val_generator,
            callbacks=callbacks,
            verbose=1
        )
        
        return history, history_fine
    
    def predict_with_features(self, image: np.ndarray) -> Dict[str, Any]:
        """Make prediction with extracted pharyngeal features"""
        # Preprocess image and extract features
        processed_image, features = self.preprocessor.preprocess_image(image)
        
        # Prepare inputs for the model: image batch and feature array
        # Expand dimensions to create a batch of 1 image
        image_batch = np.expand_dims(processed_image, axis=0)
        # Create a feature array from the extracted features, matching the input shape (1, 6)
        feature_array = np.array([[
            features['inflammation_ratio'],
            features['red_intensity'],
            features['texture_roughness'],
            features['patch_count'],
            features['patch_ratio'],
            features['swelling_complexity']
        ]])
        
        # Get model prediction
        predictions = self.model.predict([image_batch, feature_array])
        predicted_class_index = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class_index])
        
        # Generate health recommendation
        recommendation = self.generate_pharyngitis_recommendation(predicted_class_index, confidence, features)
        
        result = {
            'predicted_class': self.class_names[predicted_class_index],
            'confidence': confidence,
            'class_probabilities': {
                self.class_names[i]: float(predictions[0][i]) 
                for i in range(len(self.class_names))
            },
            'extracted_features': features,
            'recommendation': recommendation,
            'severity_score': self.calculate_severity_score(features)
        }
        
        return result
    
    def calculate_severity_score(self, features: Dict[str, float]) -> float:
        """Calculate a severity score based on extracted features"""
        score = 0.0
        
        # Assign points based on the severity indicators from features
        # Inflammation contribution (0-30 points), scaled by ratio
        score += min(features['inflammation_ratio'] * 100, 30)
        
        # Patch/exudate contribution (0-25 points), scaled by count
        score += min(features['patch_count'] * 5, 25)
        
        # Red intensity contribution (0-20 points), higher intensity means more points
        # Assuming red_intensity > 128 indicates redness, scale it
        score += min((features['red_intensity'] - 128) / 128 * 20, 20) if features['red_intensity'] > 128 else 0
        
        # Texture roughness contribution (0-15 points), higher roughness means more points
        # Scale roughness (e.g., divide by a typical max value like 1000 for scaling)
        score += min(features['texture_roughness'] / 1000 * 15, 15)
        
        # Swelling complexity contribution (0-10 points)
        score += min(features['swelling_complexity'] * 10, 10)
        
        return min(score, 100.0) # Cap the score at 100
    
    def generate_pharyngitis_recommendation(self, predicted_class_index: int, confidence: float, features: Dict[str, float]) -> str:
        """Generate health recommendations based on pharyngitis prediction and extracted features"""
        # Base recommendations for each class
        recommendations = {
            # Updated recommendations for binary classification
            0: "Your throat appears normal. Continue maintaining good oral hygiene and stay hydrated.", # Corresponds to 'no'
            1: "Pharyngitis detected. Depending on severity, consider consulting a healthcare provider. Rest, stay hydrated, gargle with warm salt water." # Corresponds to 'phar'
        }
        
        base_rec = recommendations[predicted_class_index]
        severity_score = self.calculate_severity_score(features)
        
        # Add feature-based recommendations to provide more specific advice
        if features['inflammation_ratio'] > 0.4:
            base_rec += " Significant inflammation detected - avoid irritants."
        if features['patch_count'] > 5:
            base_rec += " Multiple patches observed - possible bacterial infection."
        if features['red_intensity'] > 140:
            base_rec += " High red intensity suggests active inflammation."
        if severity_score > 70:
            base_rec += " High severity score - prioritize medical consultation."
        if confidence < 0.6:
            base_rec += " Note: Prediction confidence is moderate. Professional evaluation recommended."
            
        return base_rec
    
    def generate_gradcam(self, image: np.ndarray, class_index: int = None) -> np.ndarray:
        """Generate Grad-CAM heatmap for pharyngitis model interpretability"""
        # Preprocess image to get the normalized image and features
        processed_image, features = self.preprocessor.preprocess_image(image)
        image_batch = np.expand_dims(processed_image, axis=0)
        feature_array = np.array([[
            features['inflammation_ratio'],
            features['red_intensity'],
            features['texture_roughness'],
            features['patch_count'],
            features['patch_ratio'],
            features['swelling_complexity']
        ]])
        
        # Get the convolutional base model (EfficientNetB0)
        # It's the 3rd layer in your combined model (index 2)
        conv_model = self.model.layers[2]
        
        # Create a model that outputs both the convolutional layer activations and the final predictions
        grad_model = keras.Model(
            inputs=self.model.inputs,
            outputs=[conv_model.output, self.model.output]
        )
        
        # Compute gradients of the predicted class with respect to the feature map activations
        with tf.GradientTape() as tape:
            conv_outputs, predictions = grad_model([image_batch, feature_array])
            if class_index is None:
                # If no specific class index is provided, use the predicted class
                class_index = tf.argmax(predictions[0])
            # Get the loss for the target class
            loss = predictions[:, class_index]
        
        # Get gradients of the loss with respect to the convolutional outputs
        grads = tape.gradient(loss, conv_outputs)
        # Pool the gradients over all the axes leaving out the channel dimension
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
        
        # Multiply each channel in the feature map by the importance of that channel
        conv_outputs = conv_outputs[0] # Remove batch dimension
        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis] # Dot product
        heatmap = tf.squeeze(heatmap) # Remove the last dimension of size 1
        
        # Normalize the heatmap to values between 0 and 1
        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
        heatmap = heatmap.numpy()
        
        # Resize heatmap to match the input image size
        heatmap = cv2.resize(heatmap, (processed_image.shape[1], processed_image.shape[0]))
        
        # Convert heatmap to a color map (e.g., JET)
        heatmap = np.uint8(255 * heatmap)
        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
        
        # Overlay the heatmap on the original image
        # Convert processed_image back to uint8 for blending
        overlay = cv2.addWeighted(
            (processed_image * 255).astype(np.uint8), 0.6, # Original image (60% transparency)
            heatmap, 0.4, 0 # Heatmap (40% transparency)
        )
        
        return overlay
    
    def evaluate_model(self, test_generator: DualInputDataGenerator):
        """Evaluate pharyngitis model performance"""
        print("\nEvaluating model performance...")
        
        # Reset the test generator before making predictions to ensure it starts from the beginning
        test_generator.image_generator.reset()
        
        # Get predictions from the model
        predictions = self.model.predict(test_generator)
        predicted_classes = np.argmax(predictions, axis=1)
        
        # Get true labels directly from the underlying image generator
        # This avoids re-iterating the generator, which can cause it to be exhausted
        true_classes = test_generator.image_generator.labels
        
        # Ensure that the number of predictions matches the number of true labels
        if len(predicted_classes) != len(true_classes):
            print(f"Warning: Mismatch between number of predictions ({len(predicted_classes)}) and true labels ({len(true_classes)}). Truncating true labels.")
            true_classes = true_classes[:len(predicted_classes)]
        
        # Calculate overall accuracy
        accuracy = accuracy_score(true_classes, predicted_classes)
        
        # Generate classification report for detailed metrics (precision, recall, f1-score)
        report = classification_report(
            true_classes, predicted_classes,
            target_names=self.class_names,
            output_dict=True,
            zero_division=0 # Handle cases where a class has no true samples or no predictions
        )
        
        # Generate confusion matrix
        cm = confusion_matrix(true_classes, predicted_classes)
        
        # Calculate per-class accuracy
        # Avoid division by zero if a class has no true samples
        per_class_accuracy = {}
        for i, class_name in enumerate(self.class_names):
            if cm.sum(axis=1)[i] > 0:
                per_class_accuracy[class_name] = cm.diagonal()[i] / cm.sum(axis=1)[i]
            else:
                per_class_accuracy[class_name] = 0.0 # No samples for this class
        
        return {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'per_class_accuracy': per_class_accuracy
        }
    
    def plot_training_history(self, history, history_fine=None):
        """Plot comprehensive training history for pharyngitis model"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # Combine histories if fine-tuning was performed
        acc = history.history['accuracy']
        val_acc = history.history['val_accuracy']
        loss = history.history['loss']
        val_loss = history.history['val_loss']
        prec = history.history['precision']
        val_prec = history.history['val_precision']
        recall = history.history['recall']
        val_recall = history.history['val_recall']

        if history_fine:
            acc += history_fine.history['accuracy']
            val_acc += history_fine.history['val_accuracy']
            loss += history_fine.history['loss']
            val_loss += history_fine.history['val_loss']
            prec += history_fine.history['precision']
            val_prec += history_fine.history['val_precision']
            recall += history_fine.history['recall']
            val_recall += history_fine.history['val_recall']

        epochs_range = range(len(acc))

        # Accuracy
        axes[0, 0].plot(epochs_range, acc, label='Training Accuracy', linewidth=2)
        axes[0, 0].plot(epochs_range, val_acc, label='Validation Accuracy', linewidth=2)
        axes[0, 0].set_title('Pharyngitis Model Accuracy', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Loss
        axes[0, 1].plot(epochs_range, loss, label='Training Loss', linewidth=2)
        axes[0, 1].plot(epochs_range, val_loss, label='Validation Loss', linewidth=2)
        axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Precision
        axes[1, 0].plot(epochs_range, prec, label='Training Precision', linewidth=2)
        axes[1, 0].plot(epochs_range, val_prec, label='Validation Precision', linewidth=2)
        axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Precision')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Recall
        axes[1, 1].plot(epochs_range, recall, label='Training Recall', linewidth=2)
        axes[1, 1].plot(epochs_range, val_recall, label='Validation Recall', linewidth=2)
        axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def save_model(self, filepath: str):
        """Save the trained pharyngitis model and its metadata"""
        self.model.save(filepath)
        
        # Save additional metadata
        metadata = {
            'class_names': self.class_names,
            'input_shape': self.input_shape,
            'num_classes': self.num_classes,
            'model_type': 'pharyngitis_detection'
        }
        
        # Save metadata to a JSON file alongside the model
        with open(filepath.replace('.h5', '_metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"Model saved to {filepath} and metadata to {filepath.replace('.h5', '_metadata.json')}")
            
    def load_model(self, filepath: str):
        """Load a trained pharyngitis model and its metadata"""
        self.model = keras.models.load_model(filepath)
        
        # Load metadata
        try:
            with open(filepath.replace('.h5', '_metadata.json'), 'r') as f:
                metadata = json.load(f)
                self.class_names = metadata['class_names']
                self.input_shape = metadata['input_shape']
                self.num_classes = metadata['num_classes']
                print(f"Model loaded from {filepath} with metadata.")
        except FileNotFoundError:
            print("Metadata file not found. Using default values for class_names, input_shape, num_classes.")
        except Exception as e:
            print(f"Error loading metadata: {e}. Using default values.")


# Additional utility functions for pharyngitis dataset preparation
# Removed PharyngitisDatasetPreparator as its functions are now integrated into main or not needed.

# Demo function for testing pharyngitis detection
def demo_pharyngitis_prediction(model_path: str, image_path: str):
    """Demo function to test pharyngitis model on a single image"""
    # Load model
    model = PharyngitisDetectionModel()
    model.load_model(model_path)
    
    # Load and preprocess image
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error: Could not load image from {image_path}")
        return
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert BGR to RGB for consistent processing
    
    # Make prediction
    result = model.predict_with_features(image)
    
    print("\n--- Prediction Result ---")
    print(f"Predicted Class: {result['predicted_class']}")
    print(f"Confidence: {result['confidence']:.4f}")
    print(f"Severity Score: {result['severity_score']:.2f}")
    print("Class Probabilities:")
    for cls, prob in result['class_probabilities'].items():
        print(f"  {cls}: {prob:.4f}")
    print("\nExtracted Features:")
    for feature, value in result['extracted_features'].items():
        print(f"  {feature}: {value:.4f}")
    print(f"\nRecommendation: {result['recommendation']}")

    # Generate Grad-CAM visualization
    gradcam_overlay = model.generate_gradcam(image, class_index=model.class_names.index(result['predicted_class']))
    
    # Display original image and Grad-CAM overlay
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(image)
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(gradcam_overlay)
    plt.title(f'Grad-CAM for {result["predicted_class"]}')
    plt.axis('off')
    plt.show()


def main():
    """Main training and evaluation pipeline for pharyngitis detection"""
    
    # --- 1. Mount Google Drive and Dataset Instructions ---
    print("--- Mount Google Drive and Dataset Instructions ---")
    print("Mounting Google Drive...")
    drive.mount('/content/drive')
    print("Please ensure you have uploaded your 'archive.zip' file into your Google Drive.")
    print("A common location is the root of 'My Drive', so the path would be '/content/drive/MyDrive/archive.zip'.")
    print("If you place it in a subfolder, please update the 'zip_file_path' variable in the code accordingly.")
    
    zip_file_path = '/content/drive/MyDrive/archive.zip' # Path for Google Drive
    extract_dir = '/content/pharyngitis_dataset_extracted'
    
    # --- 2. Verify Zip File and Extract ---
    print("\n--- Verifying Zip File and Extracting Dataset ---")
    if not os.path.exists(zip_file_path):
        print(f"Error: Zip file '{zip_file_path}' not found.")
        print("Please ensure you have uploaded 'archive.zip' to the specified path in your Google Drive.")
        return

    try:
        # Create extraction directory if it doesn't exist
        os.makedirs(extract_dir, exist_ok=True)
        
        # Unzip the archive
        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        print(f"'{zip_file_path}' successfully unzipped to '{extract_dir}'.")

        # After unzipping, the dataset is expected to be in 'Pharyngitis_dataset_updated'
        # inside the 'extract_dir'. Let's explicitly set this path.
        base_data_path = os.path.join(extract_dir, 'Pharyngitis_dataset_updated')

        print(f"Expected dataset root after extraction: {base_data_path}")
        
        # Verify that the expected base_data_path actually exists
        if not os.path.exists(base_data_path):
            print(f"Error: Expected dataset directory '{base_data_path}' not found after extraction.")
            print("Please check the contents of your 'archive.zip' to confirm the top-level folder name after extraction.")
            print(f"Contents of '{extract_dir}': {os.listdir(extract_dir) if os.path.exists(extract_dir) else 'Path does not exist'}")
            return

        print(f"Contents of determined base data path '{base_data_path}': {os.listdir(base_data_path) if os.path.exists(base_data_path) else 'Path does not exist'}")


    except Exception as e:
        print(f"Error during zip extraction: {e}")
        print("Please ensure the zip file is valid and not corrupted.")
        return

    # --- 3. Initialize and Build Model ---
    print("\n--- Initializing Pharyngitis Detection Model ---")
    # Model instance will be created inside the K-Fold loop for fresh start each time
    
    # --- 4. Prepare Data for K-Fold Cross-Validation ---
    print("\n--- Preparing Data for K-Fold Cross-Validation ---")
    
    # Define the actual train directory with trailing space
    full_train_data_dir = os.path.join(base_data_path, 'train ') 
    test_data_dir = os.path.join(base_data_path, 'test ') 

    # Check if the main training directory exists
    if not os.path.exists(full_train_data_dir):
        print(f"Error: Training directory '{full_train_data_dir}' not found.")
        print("Please ensure your extracted dataset has the correct structure, including the 'train ' folder.")
        print(f"Contents of '{base_data_path}': {os.listdir(base_data_path) if os.path.exists(base_data_path) else 'Path does not exist'}")
        return

    # Get all image paths and their corresponding labels from the main training directory
    all_train_image_paths = []
    all_train_labels = []
    class_names_map = {} # To map folder names to integer labels

    # Iterate through class folders within the 'train ' directory
    for class_folder_name in os.listdir(full_train_data_dir):
        # Strip any potential trailing spaces from the class folder name for mapping
        clean_class_name = class_folder_name.strip() 
        class_dir = os.path.join(full_train_data_dir, class_folder_name)
        if os.path.isdir(class_dir):
            if clean_class_name not in class_names_map:
                class_names_map[clean_class_name] = len(class_names_map) # Assign integer label
            
            for img_name in os.listdir(class_dir):
                img_path = os.path.join(class_dir, img_name)
                if os.path.isfile(img_path) and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    all_train_image_paths.append(img_path)
                    all_train_labels.append(class_names_map[clean_class_name])

    # Ensure we have images to split
    if not all_train_image_paths:
        print(f"Error: No images found in the training directory '{full_train_data_dir}'.")
        print("Please ensure the class subfolders ('no (1)', 'phar (1)') exist and contain images.")
        return

    print(f"Total training images found for K-Fold: {len(all_train_image_paths)}")

    # Setup K-Fold Cross-Validation
    n_splits = 5 # Number of folds
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    fold_accuracies = []
    fold_precisions = []
    fold_recalls = []
    fold_confusion_matrices = []

    # Loop through each fold
    for fold, (train_index, val_index) in enumerate(skf.split(all_train_image_paths, all_train_labels)):
        print(f"\n--- Starting Fold {fold + 1}/{n_splits} ---")

        # Create temporary directories for the current fold's train and validation data
        temp_fold_train_dir = os.path.join(extract_dir, f'fold_{fold+1}_train')
        temp_fold_val_dir = os.path.join(extract_dir, f'fold_{fold+1}_validation')

        # Clean up previous temp directories for this fold if they exist
        if os.path.exists(temp_fold_train_dir):
            shutil.rmtree(temp_fold_train_dir)
        if os.path.exists(temp_fold_val_dir):
            shutil.rmtree(temp_fold_val_dir)

        # Recreate temp directories for the current fold
        os.makedirs(temp_fold_train_dir, exist_ok=True)
        os.makedirs(temp_fold_val_dir, exist_ok=True)

        # Populate temporary train and validation directories for the current fold
        idx_to_class_name = {v: k for k, v in class_names_map.items()}

        for i in train_index:
            path = all_train_image_paths[i]
            label_idx = all_train_labels[i]
            class_name = idx_to_class_name[label_idx]
            dest_dir = os.path.join(temp_fold_train_dir, class_name)
            os.makedirs(dest_dir, exist_ok=True)
            shutil.copy(path, dest_dir)

        for i in val_index:
            path = all_train_image_paths[i]
            label_idx = all_train_labels[i]
            class_name = idx_to_class_name[label_idx]
            dest_dir = os.path.join(temp_fold_val_dir, class_name)
            os.makedirs(dest_dir, exist_ok=True)
            shutil.copy(path, dest_dir)
        
        print(f"Fold {fold+1} training data created at: {temp_fold_train_dir}")
        print(f"Fold {fold+1} validation data created at: {temp_fold_val_dir}")

        # --- 5. Initialize and Build Model (for current fold) ---
        model_instance = PharyngitisDetectionModel()
        model_instance.build_model()
        print(f"Pharyngitis Detection Model Architecture for Fold {fold+1}:")
        model_instance.model.summary() # This will print for each fold, consider moving outside loop if too verbose

        # --- 6. Create Data Generators (for current fold) ---
        try:
            train_gen, val_gen = model_instance.create_data_generators(temp_fold_train_dir, temp_fold_val_dir)
            print(f"Found {train_gen.image_generator.samples} training images belonging to {train_gen.image_generator.num_classes} classes for Fold {fold+1}.")
            print(f"Found {val_gen.image_generator.samples} validation images belonging to {val_gen.image_generator.num_classes} classes for Fold {fold+1}.")
        except Exception as e:
            print(f"Error creating data generators for Fold {fold+1}: {e}")
            print(f"Please verify that '{temp_fold_train_dir}' and '{temp_fold_val_dir}' contain subfolders like 'no (1)', 'phar (1)', etc., with images inside.")
            # Clean up temp directories before exiting on error
            if os.path.exists(temp_fold_train_dir):
                shutil.rmtree(temp_fold_train_dir)
            if os.path.exists(temp_fold_val_dir):
                shutil.rmtree(temp_fold_val_dir)
            return

        # --- 7. Train Model (for current fold) ---
        print(f"\n--- Starting Model Training for Fold {fold + 1} ---")
        history, history_fine = model_instance.train_model(train_gen, val_gen, epochs=60)
        
        # --- 8. Plot Training History (for current fold) ---
        print(f"\n--- Plotting Training History for Fold {fold + 1} ---")
        model_instance.plot_training_history(history, history_fine)
        
        # --- 9. Save Model (for current fold) ---
        fold_model_save_path = f'pharyngitis_detection_model_fold_{fold+1}.h5'
        model_instance.save_model(fold_model_save_path)
        
        # --- 10. Evaluate Model on Test Data (for current fold) ---
        print(f"\n--- Evaluating Model on Test Data for Fold {fold + 1} ---")
        if os.path.exists(test_data_dir) and any(os.listdir(test_data_dir)):
            # Create a test generator (no augmentation, only rescaling)
            test_datagen = ImageDataGenerator(rescale=1./255)
            test_image_generator = test_datagen.flow_from_directory(
                test_data_dir,
                target_size=model_instance.input_shape[:2],
                batch_size=32,
                class_mode='categorical',
                shuffle=False # Important for evaluation to maintain order
            )
            # Create the dual input test generator
            test_gen_dual = DualInputDataGenerator(test_image_generator, model_instance.preprocessor) 

            evaluation_results = model_instance.evaluate_model(test_gen_dual)
            print(f"\n--- Model Evaluation Results for Fold {fold + 1} ---")
            print(f"Overall Accuracy: {evaluation_results['accuracy']:.4f}")
            print("\nClassification Report:")
            print(pd.DataFrame(evaluation_results['classification_report']).transpose())
            print("\nConfusion Matrix:")
            print(evaluation_results['confusion_matrix'])
            print("\nPer-Class Accuracy:")
            for cls, acc in evaluation_results['per_class_accuracy'].items():
                print(f"  {cls}: {acc:.4f}")
            
            fold_accuracies.append(evaluation_results['accuracy'])
            fold_precisions.append(evaluation_results['classification_report']['weighted avg']['precision'])
            fold_recalls.append(evaluation_results['classification_report']['weighted avg']['recall'])
            fold_confusion_matrices.append(evaluation_results['confusion_matrix'])

        else:
            print(f"No test data found at {test_data_dir}. Skipping final evaluation for Fold {fold+1}.")

        # Clean up temporary directories for the current fold
        if os.path.exists(temp_fold_train_dir):
            shutil.rmtree(temp_fold_train_dir)
        if os.path.exists(temp_fold_val_dir):
            shutil.rmtree(temp_fold_val_dir)
        print(f"Cleaned up temporary directories for Fold {fold+1}.")

    print("\n--- K-Fold Cross-Validation Complete! ---")
    if fold_accuracies:
        print(f"\nAverage Test Accuracy across {n_splits} folds: {np.mean(fold_accuracies):.4f} +/- {np.std(fold_accuracies):.4f}")
        print(f"Average Test Precision across {n_splits} folds: {np.mean(fold_precisions):.4f} +/- {np.std(fold_precisions):.4f}")
        print(f"Average Test Recall across {n_splits} folds: {np.mean(fold_recalls):.4f} +/- {np.std(fold_recalls):.4f}")
        
        # You can also aggregate confusion matrices if needed
        # For simplicity, we'll just print the average metrics.

    print("\nPharyngitis Detection Model training and evaluation pipeline complete!")
    print("\nKey Features of your model:")
    print("- 2-class classification (no (1), phar (1))") 
    print("- Advanced pharyngeal feature extraction (inflammation, patches, swelling)")
    print("- Dual-input architecture (image + medical features for enhanced accuracy)")
    print("- Severity scoring system for clinical insights")
    print("- Grad-CAM visualization for model interpretability")
    print("- K-Fold Cross-Validation for robust performance estimation")
    print("\nTo use the trained model for prediction on a new image:")
    print(f"Load it using: `model_instance.load_model('best_pharyngitis_model_fold_X.h5')` (replace X with the desired fold number)")
    print("Then call: `model_instance.predict_with_features(your_image_array)`")

if __name__ == "__main__":
    main()
